{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "\n",
    "warnings.filterwarnings( 'ignore' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importação de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_03 = pd.read_pickle('../exports/cicle_exports/03_filtering_to_business/df_03.pkl', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_05 = df_03.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ohe(encoder ,encoded_array, variable_name, df):\n",
    "    '''\n",
    "    Aplica o One Hot Encoder no dataframe, criando as colunas respectivas às categorias no encoder e elimininado a coluna original.\n",
    "\n",
    "    Parâmetros:\n",
    "        encoder (OneHotEncoder): Encoder OneHotEncoder do Scikit-learn pré-configurado.\n",
    "        encoded_array (ndarray): Array gerado pelo OneHotEncoder.\n",
    "        variable_name (str): Nome da variável original.\n",
    "        df (DataFrame): Dataframe a ser alterado.\n",
    "\n",
    "    Retorna:\n",
    "        Um novo dataframe com as novas colunas respectivas às categorias no encoder e sem a coluna original.\n",
    "    '''\n",
    "\n",
    "    # Criar DataFrame com nomes corretos\n",
    "    encoded_df = pd.DataFrame(\n",
    "        encoded_array,\n",
    "        columns=encoder.get_feature_names_out([variable_name]),\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    # Concatenar com o DataFrame original, removendo a coluna original\n",
    "    df_f = pd.concat([df.drop(columns=[variable_name]), encoded_df], axis=1)\n",
    "\n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVT - Separação entre treino, validação e teste (70% - 15% - 15%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A separação será feita respeitando o ordenamento temporal, pois trata-se de um caso de **Série Temporal**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando dataframe com datas únicas\n",
    "unique_days = df_05[['year', 'month', 'day']].drop_duplicates()\n",
    "unique_days = unique_days.sort_values(['year', 'month', 'day']).reset_index(drop=True)\n",
    "\n",
    "# Calculando o tamanho de cada divisão\n",
    "n = len(unique_days)\n",
    "train_end = int(n * 0.7)\n",
    "val_end = train_end + int(n * 0.15)\n",
    "\n",
    "# Obtendo as semanas para cada divisão\n",
    "train_days = unique_days.iloc[:train_end]\n",
    "val_days = unique_days.iloc[train_end:val_end]\n",
    "test_days = unique_days.iloc[val_end:]\n",
    "\n",
    "# Criando os dataframes com base nas semanas\n",
    "def filter_days(df_05, days):\n",
    "    return df_05.merge(days, on=['year', 'month', 'day'], how='inner')\n",
    "\n",
    "df_05_train = filter_days(df_05, train_days)\n",
    "df_05_val = filter_days(df_05, val_days)\n",
    "df_05_test = filter_days(df_05, test_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0. Prepararação dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Normalizacao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não há dados a serem normalizados, pois não há variáveis numéricas que possuem um comportamento de distribuição normal (gausiano). Vide passo 4 - análise exploratória."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competition_distance\n",
    "rs_competition_distance = RobustScaler() # Reescalador tratador de outliers\n",
    "df_05_train['competition_distance'] = rs_competition_distance.fit_transform( df_05_train[['competition_distance']].values )\n",
    "\n",
    "# competition_time_month\n",
    "rs_competition_time_month = RobustScaler() # Reescalador tratador de outliers\n",
    "df_05_train['competition_time_month'] = rs_competition_time_month.fit_transform( df_05_train[['competition_time_month']].values )\n",
    "\n",
    "# promo_time_week\n",
    "mms_promo_time_week = MinMaxScaler() # Reescalador normal\n",
    "df_05_train['promo_time_week'] = mms_promo_time_week.fit_transform( df_05_train[['promo_time_week']].values )\n",
    "\n",
    "# year\n",
    "mms_year = MinMaxScaler() # Reescalador normal\n",
    "df_05_train['year'] = mms_year.fit_transform( df_05_train[['year']].values )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Transformação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1. Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_holiday - One Hot Encoding\n",
    "ohe_state_holiday = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "encoded_array = ohe_state_holiday.fit_transform(df_05_train[['state_holiday']])\n",
    "df_05_train = apply_ohe(ohe_state_holiday, encoded_array, 'state_holiday', df_05_train)\n",
    "\n",
    "# store_type - Label Encoding\n",
    "le_store_type = LabelEncoder()\n",
    "df_05_train['store_type'] = le_store_type.fit_transform( df_05_train[['store_type']].values )\n",
    "\n",
    "# assortment - Ordinal Encoding\n",
    "assortment_dict = {'basic': 1,  'extra': 2, 'extended': 3}\n",
    "df_05_train['assortment'] = df_05_train['assortment'].map( assortment_dict )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2. Transformação da variável resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando escala logarítimica:\n",
    "df_05_train['sales'] = np.log1p( df_05_train['sales'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3. Transformação de natureza (encoder cíclico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day of week\n",
    "df_05_train['day_of_week_sin'] = df_05_train['day_of_week'].apply( lambda x: np.sin( x * ( 2. * np.pi/7 ) ) )\n",
    "df_05_train['day_of_week_cos'] = df_05_train['day_of_week'].apply( lambda x: np.cos( x * ( 2. * np.pi/7 ) ) )\n",
    "\n",
    "# month\n",
    "df_05_train['month_sin'] = df_05_train['month'].apply( lambda x: np.sin( x * ( 2. * np.pi/12 ) ) )\n",
    "df_05_train['month_cos'] = df_05_train['month'].apply( lambda x: np.cos( x * ( 2. * np.pi/12 ) ) )\n",
    "\n",
    "# day \n",
    "df_05_train['day_sin'] = df_05_train['day'].apply( lambda x: np.sin( x * ( 2. * np.pi/30 ) ) )\n",
    "df_05_train['day_cos'] = df_05_train['day'].apply( lambda x: np.cos( x * ( 2. * np.pi/30 ) ) )\n",
    "\n",
    "# week of year\n",
    "df_05_train['week_of_year_sin'] = df_05_train['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi/52 ) ) )\n",
    "df_05_train['week_of_year_cos'] = df_05_train['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi/52 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Descartando colunas antigas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = ['week_of_year', 'day', 'month', 'day_of_week', 'promo_since', 'competition_since', 'year_week' ]\n",
    "df_05_train = df_05_train.drop( cols_drop, axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. Aplicando data preparation nos dados de validação e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_data_preparation(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Aplica todas as transformações da etapa de data preparation no dataframe inserido.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame para aplicar transformações.\n",
    "\n",
    "    \"\"\"\n",
    "    # Normalização - Não há dados\n",
    "\n",
    "    df_f = df.copy()\n",
    "\n",
    "    # Rescaling\n",
    "\n",
    "    # competition_distance\n",
    "    df_f['competition_distance'] = rs_competition_distance.transform( df_f[['competition_distance']].values )\n",
    "\n",
    "    # competition_time_month\n",
    "    df_f['competition_time_month'] = rs_competition_time_month.transform( df_f[['competition_time_month']].values )\n",
    "\n",
    "    # promo_time_week\n",
    "    df_f['promo_time_week'] = mms_promo_time_week.transform( df_f[['promo_time_week']].values )\n",
    "\n",
    "    # year\n",
    "    df_f['year'] = mms_year.transform( df_f[['year']].values )\n",
    "\n",
    "\n",
    "    # Encoding\n",
    "\n",
    "    # state_holiday - One Hot Encoding\n",
    "    encoded_array = ohe_state_holiday.transform(df_f[['state_holiday']])\n",
    "    df_f = apply_ohe(ohe_state_holiday, encoded_array, 'state_holiday', df_f)\n",
    "\n",
    "    # store_type - Label Encoding\n",
    "    df_f['store_type'] = le_store_type.transform( df_f[['store_type']].values )\n",
    "\n",
    "    # assortment - Ordinal Encoding\n",
    "    assortment_dict = {'basic': 1,  'extra': 2, 'extended': 3}\n",
    "    df_f['assortment'] = df_f['assortment'].map( assortment_dict )\n",
    "\n",
    "\n",
    "    # Transformação da variável resposta\n",
    "\n",
    "    # Aplicando escala logarítimica:\n",
    "    df_f['sales'] = np.log1p( df_f['sales'] )\n",
    "\n",
    "\n",
    "    # Transformação de natureza (encoder cíclico)\n",
    "\n",
    "    # day of week\n",
    "    df_f['day_of_week_sin'] = df_f['day_of_week'].apply( lambda x: np.sin( x * ( 2. * np.pi/7 ) ) )\n",
    "    df_f['day_of_week_cos'] = df_f['day_of_week'].apply( lambda x: np.cos( x * ( 2. * np.pi/7 ) ) )\n",
    "\n",
    "    # month\n",
    "    df_f['month_sin'] = df_f['month'].apply( lambda x: np.sin( x * ( 2. * np.pi/12 ) ) )\n",
    "    df_f['month_cos'] = df_f['month'].apply( lambda x: np.cos( x * ( 2. * np.pi/12 ) ) )\n",
    "\n",
    "    # day \n",
    "    df_f['day_sin'] = df_f['day'].apply( lambda x: np.sin( x * ( 2. * np.pi/30 ) ) )\n",
    "    df_f['day_cos'] = df_f['day'].apply( lambda x: np.cos( x * ( 2. * np.pi/30 ) ) )\n",
    "\n",
    "    # week of year\n",
    "    df_f['week_of_year_sin'] = df_f['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi/52 ) ) )\n",
    "    df_f['week_of_year_cos'] = df_f['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi/52 ) ) )\n",
    "\n",
    "\n",
    "    # Descartando colunas antigas\n",
    "\n",
    "    cols_drop = ['week_of_year', 'day', 'month', 'day_of_week', 'promo_since', 'competition_since', 'year_week' ]\n",
    "    df_f = df_f.drop( cols_drop, axis=1 )\n",
    "\n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_05_val = apply_data_preparation(df_05_val)\n",
    "df_05_test = apply_data_preparation(df_05_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportação de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_05_train.to_pickle   ('../exports/cicle_exports/05_data_preparation/df_05_train.pkl', compression='gzip')\n",
    "df_05_val.to_pickle     ('../exports/cicle_exports/05_data_preparation/df_05_val.pkl', compression='gzip')\n",
    "df_05_test.to_pickle    ('../exports/cicle_exports/05_data_preparation/df_05_test.pkl', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exportação para produto final:**\n",
    "\n",
    "Todos os encoders e scalers que necessitam de um processo de fit serão exportados para não haver necessidade de repetir o processo de fit durante a produção. Portanto esses arquivos farão parte do produto final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rs_competition_distance,    open( '../exports/cicle_products/rs_competition_distance.pkl', 'wb'))\n",
    "pickle.dump(rs_competition_time_month,  open( '../exports/cicle_products/rs_competition_time_month.pkl', 'wb'))\n",
    "\n",
    "pickle.dump(mms_promo_time_week,        open( '../exports/cicle_products/mms_promo_time_week.pkl', 'wb'))\n",
    "pickle.dump(mms_year,                   open( '../exports/cicle_products/mms_year.pkl', 'wb'))\n",
    "\n",
    "pickle.dump(ohe_state_holiday,          open( '../exports/cicle_products/ohe_state_holiday.pkl', 'wb'))\n",
    "\n",
    "pickle.dump(le_store_type,              open( '../exports/cicle_products/le_store_type.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ros",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
